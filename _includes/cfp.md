## Call for Contributions

We invite you to submit papers (up to 9 pages for long papers and up to 5 pages for short papers, excluding references and appendix) in the <INSERT FORMAT> format. All submissions will be managed through <REVIEW PLATFORM>. The final submission including main paper, references and appendix should not exceed 12 pages. Supplementary Materials uploads are to only be used optionally for extra videos/code/data/figures and should be uploaded separately in the submission website.

The review process is double-blind so the submission should be anonymized. Accepted work will be presented as posters during the workshop, and select contributions will be invited to give spotlight talks during the workshop. Each accepted work entering the poster sessions will have an accompanying pre-recorded 5-minute video. Please note that at least one coauthor of each accepted paper will be expected to have a NeurIPS conference registration and participate in one of the poster sessions. 

Submissions will be evaluated based on novelty, rigor, and relevance to the theme of the workshop. Both empirical and theoretical contributions are welcome. Submissions should not have previously appeared in a journal or conference (including accepted papers to NeurIPS 2022). Submissions must adhere to the NeurIPS Code of Conduct.

The focus of the work should relate to the list of the topics specified below. The review process will be double-blind and accepted submissions will be presented as virtual talks or posters. There will be no proceedings for this workshop, however, authors can opt to have their abstracts/papers posted on the workshop website.

We encourage submissions on the following topics from the focus of bridging different perspectives on attention:
- Relationships between biological and artificial attention
- Attention for reinforcement learning and decision making
- Attention for continual / lifelong learning
- Attention as a tool for interpretation and explanation
- Attention in human-computer interaction
- Attention mechanisms in Deep Neural Network (DNN) architectures




