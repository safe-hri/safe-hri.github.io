Recent advances in Machine Learning (ML) have led to significant strides in robotics research and interactive robot learning. However, safety and normative behaviors remain critical factors in deploying these ML approaches to real-world robotic applications in unpredictable, human-populated environments. Ensuring physical safety (i.e., avoiding unwanted or harmful contact) has increasingly received attention in control, motion planning, ML, and decision-making research. Typically, robot safety is strictly related to avoiding possible injuries by the robot, but it has become apparent that robots should also be well-perceived to ensure good human-robot interaction (HRI).

A growing body of work in HRI on the cognitive aspects of safety has increased our understanding of how safe and comfortable people feel interacting with or around robots, i.e., perceived safety. Yet, physical and perceived safety have often not been studied together. Additionally, robots can increase people's safety by taking over dirty and dangerous tasks, such as heavy lifting to prevent negative health consequences for workers. However, more research is needed to explore in what contexts and at what times robots can safely take over these tasks, as well as methods to ensure people do not over- or under-rely on them. 

Recently, Foundation Models (FMs), such as GPT and Gemini, have revolutionized machine learning. However, they remain prone to issues such as hallucinations and jailbreaking, posing significant challenges for mission-critical applications (e.g., healthcare and disaster response). As FMs find more widespread applications, ensuring both physical and perceived safety becomes increasingly critical. 

This workshop aims to go beyond the typical notion of robot safety, discussing the challenges of ensuring safety and normative behaviors in real-world HRI and exploring ways to increase safety in and through HRI. We will discuss research methodologies, metrics, and modeling approaches and propose new and debate existing metrics to accelerate safe and normative HRI research. This workshopâ€™s intended outcome is to form a community of researchers spanning physical safety, perceived safety, and normative behaviors in HRI. We welcome researchers from robotics, robot learning, foundation models, formal methods, HRI, human factors, design, and psychology communities and anyone interested in robot safety and normative behaviors in and through HRI. This full-day workshop is designed to start a thought-provoking conversation around safety and normative behaviors in HRI and foster interaction between researchers from different communities and with different research foci. The program reflects this objective by giving space to researchers in industry and academia from different seniority levels and research topics, including robotics, human-robot interaction, large language models, design, and formal methods.


## Topics of Interest
Example areas of interest include but are not limited to:
* Safe robot learning.
* Formal safety guarantees, safety analysis, and safety specifications for HRI.
* Technical advances to improve perceived safety while ensuring physical safety.
* Robotic solutions to improve people's safety in daily life.
* Appropriate reliance on robots, when (not to) use them.
* Safe deployment and robustness evaluation of foundation models in robotics.
* Interpretability and explainable artificial intelligence methods to facilitate safety.
* Legibility and predictability of robot behavior.
* Types of normative robot behaviors and how they impact HRI.
* Addressing bias and fairness in foundation model-driven robot behaviors.
* Multimodal communication and task adaptation through foundation models.
* Factors that contribute to perceived safety in HRI.
* Experimental paradigms to study perceived safety and ethical considerations.
* The influence of social and cultural norms, and individual differences on safety.

