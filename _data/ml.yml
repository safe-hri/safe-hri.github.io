- title: Attention is All You Need
  author: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin
  year: 2017
  venue: NeurIPS
  link: https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf

# - title: Human Gaze Assisted Artificial Intelligence -- A Review
#   author: Ruohan Zhang, Akanksha Saran, Bo Liu, Yifeng Zhu, Sihang Guo, Scott Niekum, Dana H. Ballard, Mary M. Hayhoe
#   year: 2020
#   venue: IJCAI
#   link: https://www.ijcai.org/Proceedings/2020/0689.pdf

# - title: Neural nets learn to program neural nets with fast weights—like today's Transformer variants
#   author: Jürgen Schmidhuber
#   year: 2020
#   venue: AI Blog, Jürgen Schmidhuber
#   link: https://people.idsia.ch/~juergen/fast-weight-programmer-1991-transformer.html

# - title: End-to-End Differentiable Sequential Neural Attention
#   author: Jürgen Schmidhuber
#   year: 2021
#   venue: AI Blog, Jürgen Schmidhuber
#   link: https://people.idsia.ch/~juergen/neural-attention-1990-1993.html

# - title: Efficiently Guiding Imitation Learning Agents with Human Gaze
#   author: Akanksha Saran, Ruohan Zhang, Elaine S. Short, and Scott Niekum
#   year: 2021
#   venue: AAMAS
#   link: https://www.ifaamas.org/Proceedings/aamas2021/pdfs/p1109.pdf

# - title: Self-supervised Attention-aware Reinforcement Learning
#   author: Haiping Wu, Khimya Khetarpal, and Doina Precup
#   year: 2021
#   venue: AAAI 
#   link: https://ojs.aaai.org/index.php/AAAI/article/view/17235/17042 

# - title: Guaranteed Discovery of Controllable Latent States with Multi-Step Inverse Models
#   author: Alex Lamb, Riashat Islam, Yonathan Efroni, Aniket Didolkar, Dipendra Misra, Dylan Foster, Lekan Molu, Rajan Chari, Akshay Krishnamurthy, John Langford
#   year: 2022
#   venue: arXiv
#   link: https://arxiv.org/pdf/2207.08229.pdf

# - title: Attend before you act -- Leveraging human visual attention for continual learning
#   author: Khimya Khetarpal, and Doina Precup
#   year: 2018
#   venue: ICML Workshop on Lifelong Learning - A Reinforcement Learning Approach
#   link: https://arxiv.org/pdf/1807.09664.pdf

# - title: Transformers generalize differently from information stored in context vs in weights
#   author: Stephanie Chan, Ishita Dasgupta, Junkyung Kim, Dharshan Kumaran, Andrew K. Lampinen, and Felix Hill
#   year: 2022
#   venue: arXiv
#   link: https://arxiv.org/pdf/2210.05675.pdf

# - title: In-context Reinforcement Learning with Algorithm Distillation
#   author: Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, DJ Strouse, Steven Hansen, Angelos Filos, Ethan Brooks, Maxime Gazeau, Himanshu Sahni, Satinder Singh, Volodymyr Mnih
#   year: 2022
#   venue: arXiv
#   link: https://arxiv.org/abs/2210.14215

# - title: Wide Attention Is The Way Forward For Transformers
#   author: Jason Ross Brown, Yiren Zhao, Ilia Shumailov, and Robert D. Mullins.
#   year: 2022
#   venue: arXiv
#   link: https://arxiv.org/pdf/2210.00640.pdf

# - title: Transformers Learn Shortcuts to Automata
#   author: Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, Cyril Zhang
#   year: 2022
#   venue: arXiv
#   link: https://arxiv.org/pdf/2210.10749.pdf


